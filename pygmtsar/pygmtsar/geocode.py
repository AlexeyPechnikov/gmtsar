#!/usr/bin/env python3
# Alexey Pechnikov, Sep, 2021, https://github.com/mobigroup/gmtsar
from .datagrid import datagrid

class geocode(datagrid):

    def intf_ra2ll(self, subswath=None, grids=None, debug=False):
        """
        Geocoding function based on interferogram geocode matrix to call from open_grids(geocode=True)
        """
        from tqdm.auto import tqdm
        import joblib
        import xarray as xr
        import numpy as np
        import os

        # that's possible to miss the first argument subswath
        assert subswath is not None or grids is not None, 'ERROR: define input grids'
        if grids is None:
            grids = subswath
            subswath = None

        # helper check
        if not 'y' in grids.dims and 'x' in grids.dims:
            print ('NOTE: the grid is not in radar coordinates, miss geocoding')
            return grids

        # check if subswath exists or return a single subswath for None
        subswath = self.get_subswath(subswath)

        intf_ra2ll_file = os.path.join(self.basedir, f'F{subswath}_intf_ra2ll.grd')
        intf_ll2ra_file = os.path.join(self.basedir, f'F{subswath}_intf_ll2ra.grd')

        matrix_ra2ll = xr.open_dataarray(intf_ra2ll_file, engine=self.engine, chunks=self.chunksize)
        matrix_ll2ra = xr.open_dataarray(intf_ll2ra_file, engine=self.engine, chunks=self.chunksize)

        # conversion works for a different 1st grid dimension size
        def ra2ll(grid):
            # input and transform grids should be the same
            grid = grid.reindex_like(matrix_ll2ra)
            # some interferograms have different y dimension and matrix has the largest
            # crop matrix y dimension when it is larger than current interferogram
            matrix_ra2ll_valid = xr.where(matrix_ra2ll<grid.size, matrix_ra2ll, -1)
            da_ll = xr.DataArray(np.where(matrix_ra2ll>=0, grid.values.reshape(-1)[matrix_ra2ll_valid], np.nan),
                coords=matrix_ra2ll_valid.coords)
            return da_ll

    #        def ra2ll(grid):
    #            return xr.DataArray(np.where(matrix_ra2ll>=0, grid.values.reshape(-1)[matrix_ra2ll], np.nan),
    #                coords=matrix_ra2ll.coords)

        # process single 2D raster
        if len(grids.dims) == 2:
            return ra2ll(grids)

        # process a set of 2D rasters
        with self.tqdm_joblib(tqdm(desc='Geocoding', total=len(grids))) as progress_bar:
            grids_ll = joblib.Parallel(n_jobs=-1)(joblib.delayed(ra2ll)(grids[item]) for item in range(len(grids)))
        grids_ll = xr.concat(grids_ll, dim=grids.dims[0])

        # add coordinates from original grids
        for coord in grids.coords:
            if coord in ['y', 'x']:
                continue
            grids_ll[coord] = grids[coord]

        return grids_ll

    def intf_ra2ll_matrix(self, subswath, intf_grids, debug=False):
        """
        Build interferogram geocoding matrix after interferogram processing required for open_grids(geocode=True)
        """
        from scipy.spatial import cKDTree
        import xarray as xr
        import numpy as np
        import os

        # use 2D grid grom the pairs stack
        # sometimes interferogram grids are different for one azimuth line so use the largest grid
        intf_grid = intf_grids.min('pair')

        trans_ra2ll_file = os.path.join(self.basedir, f'F{subswath}_trans_ra2ll.grd')
        intf_ra2ll_file  = os.path.join(self.basedir, f'F{subswath}_intf_ra2ll.grd')

        # trans.dat - file generated by llt_grid2rat (r a topo lon lat)"
        trans = self.get_trans_dat(subswath)
        lon_min, lon_max = trans[:,3].min(),trans[:,3].max()
        lat_min, lat_max = trans[:,4].min(),trans[:,4].max()

        # read translation table for the full DEM area
        trans_ra2ll = xr.open_dataarray(trans_ra2ll_file, engine=self.engine, chunks=self.chunksize)

        # build ra2ll translation matrix for interferogram coordinates and area only
        # each lan/lon cell has zero or one neighbour radar cell
        # each radar cell has one or multiple (on borders) neighbour lat/lon cells
        intf_ys, intf_xs = xr.broadcast(intf_grids.y, intf_grids.x)
        intf_yxs = np.stack([intf_ys.values.reshape(-1),intf_xs.values.reshape(-1)], axis=1)
        trans_yxs = np.stack([trans[:,1],trans[:,0]], axis=1)

        tree = cKDTree(intf_yxs, compact_nodes=False, balanced_tree=False)
        # use accurate distance limit as a half of the cell diagonal
        dy = intf_grids.y.diff('y')[0]
        dx = intf_grids.x.diff('x')[0]
        distance_limit = np.sqrt((dx/2.)**2 + (dy/2.)**2) + 1e-2
        d, inds = tree.query(trans_yxs, k = 1, distance_upper_bound=distance_limit, workers=8)

        # single integer index mask
        intf2trans = np.where(~np.isinf(d), inds, -1)
        # produce the same output array
        intf_ra2ll = xr.zeros_like(trans_ra2ll).rename('intf_ra2ll')
        intf_ra2ll.values = np.where(trans_ra2ll>=0, intf2trans[trans_ra2ll], -1)
        #assert intf_grid.size - 1 == intf_ra2ll.max(), 'ERROR: transform matrix and interferograms largest grid are different'
        assert intf_grid.size > intf_ra2ll.max(), \
            f'ERROR: transform matrix size {intf_grid.size} is too small for interferograms largest index {intf_ra2ll.max()}'
        # magic: add GMT attribute to prevent coordinates shift for 1/2 pixel
        intf_ra2ll.attrs['node_offset'] = 1
        # save to NetCDF file
        if os.path.exists(intf_ra2ll_file):
            os.remove(intf_ra2ll_file)
        intf_ra2ll.to_netcdf(intf_ra2ll_file, encoding={'intf_ra2ll': self.compression}, engine=self.engine)

    def ra2ll(self, subswath, debug=False):
        """
        Create radar to geographic coordinate transformation matrix for DEM grid using geocoding table trans.dat
        """
        from scipy.spatial import cKDTree
        import xarray as xr
        import numpy as np
        import os

        trans_ra2ll_file = os.path.join(self.basedir, f'F{subswath}_trans_ra2ll.grd')

        if os.path.exists(trans_ra2ll_file):
            os.remove(trans_ra2ll_file)

        # trans.dat - file generated by llt_grid2rat (r a topo lon lat)"
        trans = self.get_trans_dat(subswath)
        lon_min, lon_max = trans[:,3].min(),trans[:,3].max()
        lat_min, lat_max = trans[:,4].min(),trans[:,4].max()

        #dem = xr.open_dataset(in_dem_gridfile)
        #dem = self.get_dem(geoloc=True)
        dem = self.get_dem(geoloc=True).sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))

        trans_latlons = np.stack([trans[:,4],trans[:,3]], axis=1)
        dem_lats, dem_lons = xr.broadcast(dem.lat,dem.lon)
        dem_latlons = np.stack([dem_lats.values.reshape(-1),dem_lons.values.reshape(-1)], axis=1)

        tree = cKDTree(trans_latlons, compact_nodes=False, balanced_tree=False)
        # use accurate distance limit as a half of the cell diagonal
        dlat = dem.lat.diff('lat')[0]
        dlon = dem.lon.diff('lon')[0]
        distance_limit = np.sqrt((dlat/2.)**2 + (dlon/2.)**2) + 1e-6
        d, inds = tree.query(dem_latlons, k = 1, distance_upper_bound=distance_limit, workers=8)

        # produce the same output array as dataset to be able to add global attributes
        trans_ra2ll = xr.zeros_like(dem).rename('trans_ra2ll')
        trans_ra2ll.values = np.where(~np.isinf(d), inds, -1).reshape(dem.shape)
        # magic: add GMT attribute to prevent coordinates shift for 1/2 pixel
        #trans_ra2ll.attrs['node_offset'] = 1
        # save to NetCDF file
        trans_ra2ll.to_netcdf(trans_ra2ll_file, encoding={'trans_ra2ll': self.compression}, engine=self.engine)

    # a single-step translation
    # see also two-step translation ra2ll & intf_ra2ll_matrix
    def intf_ll2ra_matrix(self, subswath, intf_grids, debug=False):
        """
        Create geographic to radar coordinate transformation matrix for DEM grid (F?_trans_ra2ll.grd)
        """
        from scipy.spatial import cKDTree
        import xarray as xr
        import numpy as np
        import os

        # use 2D grid grom the pairs stack
        # sometimes interferogram grids are different for one azimuth line so use the largest grid
        intf_grid = intf_grids.min('pair')

        trans_ra2ll_file = os.path.join(self.basedir, f'F{subswath}_trans_ra2ll.grd')
        intf_ll2ra_file  = os.path.join(self.basedir, f'F{subswath}_intf_ll2ra.grd')

        # to resolve opened NetCDF rewriting error
        if os.path.exists(intf_ll2ra_file):
            os.remove(intf_ll2ra_file)

        # read translation table for the full DEM area
        trans_ra2ll = xr.open_dataarray(trans_ra2ll_file, engine=self.engine, chunks=self.chunksize)

        # trans.dat - file generated by llt_grid2rat (r a topo lon lat)"
        trans = self.get_trans_dat(subswath)
        # convert trans lat, lon grid to the convertion table
        trans = trans[trans_ra2ll.values.reshape(-1),:]

        # the same indexing as in lat, lon conversion grid
        trans_yxs = np.stack([trans[:,1], trans[:,0]], axis=1)
        intf_ys, intf_xs = xr.broadcast(intf_grid.y, intf_grid.x)
        intf_yxs = np.stack([intf_ys.values.reshape(-1), intf_xs.values.reshape(-1)], axis=1)

        tree = cKDTree(trans_yxs, compact_nodes=False, balanced_tree=False)
        # use accurate distance limit as a half of the cell diagonal
        dy = intf_grid.y.diff('y')[0]
        dx = intf_grid.x.diff('x')[0]
        #distance_limit = np.sqrt((dy/2.)**2 + (dx/2.)**2) + 1e-2
        distance_limit = 100
        d, inds = tree.query(intf_yxs, k = 1, distance_upper_bound=distance_limit, workers=8)

        # produce the same output array as dataset to be able to add global attributes
        trans_ll2ra = xr.zeros_like(intf_grid).rename('intf_ll2ra')
        trans_ll2ra.values = np.where(~np.isinf(d), inds, -1).reshape(intf_grid.shape)

        if debug:
            # possible for merged subswaths due to subswaths offset
            undefined = (trans_ll2ra==-1).sum().item()
            print (f'DEBUG: inverse geocoding matrix has {undefined} undefined indices')
        # magic: add GMT attribute to prevent coordinates shift for 1/2 pixel
        trans_ll2ra.attrs['node_offset'] = 1
        # save to NetCDF file
        trans_ll2ra.to_netcdf(intf_ll2ra_file, encoding={'intf_ll2ra': self.compression}, engine=self.engine)

        return

    def intf_ll2ra(self, subswath=None, grids=None):
        """
        Inverse geocoding function based on interferogram geocode matrix to call from open_grids(inverse_geocode=True)
        """
        from tqdm.auto import tqdm
        import joblib
        import xarray as xr
        import numpy as np
        import os
        
        # that's possible to miss the first argument subswath
        assert subswath is not None or grids is not None, 'ERROR: define input grids'
        if grids is None:
            grids = subswath
            subswath = None
        
        # helper check
        if not 'lat' in grids.dims and 'lon' in grid.dims:
            print ('NOTE: the grid is not in geograpphic coordinates, miss geocoding')
            return grids

        # check if subswath exists or return a single subswath for None
        subswath = self.get_subswath(subswath)
            
        trans_ra2ll_file = os.path.join(self.basedir, f'F{subswath}_trans_ra2ll.grd')
        intf_ll2ra_file = os.path.join(self.basedir, f'F{subswath}_intf_ll2ra.grd')

        # read translation table for the full DEM area
        trans_ra2ll = xr.open_dataarray(trans_ra2ll_file, engine=self.engine, chunks=self.chunksize)
        # transform matrix
        matrix_ll2ra = xr.open_dataarray(intf_ll2ra_file, engine=self.engine, chunks=self.chunksize)

        def ll2ra(grid):
            # transform input grid to the trans_ra2ll where the geocoding matrix defined
            # only nearest interpolation allowed to save values of binary masks
            return xr.DataArray(np.where(matrix_ll2ra>=0,
                                         grid.interp_like(trans_ra2ll, method='nearest').values.reshape(-1)[matrix_ll2ra],
                                         np.nan),
                coords=matrix_ll2ra.coords)

        # process single 2D raster
        if len(grids.dims) == 2:
            return ll2ra(grids)

        # process a set of 2D rasters
        with self.tqdm_joblib(tqdm(desc='Geocoding', total=len(grids))) as progress_bar:
            grids_ll = joblib.Parallel(n_jobs=-1)(joblib.delayed(ll2ra)(grids[item]) for item in range(len(grids)))
        grids_ll = xr.concat(grids_ll, dim=grids.dims[0])

        # add coordinates from original grids
        for coord in grids.coords:
            if coord in ['lat', 'lon']:
                continue
            grids_ll[coord] = grids[coord]

        return grids_ll

    def get_trans_dat_blocks_extents(self, subswath=None, n_jobs=-1):
        """
        Compute trans_dat dask blocks extents in radar coordinates
        """
        from tqdm.auto import tqdm
        import joblib
        import xarray as xr
        import dask
        import numpy as np

        # range, azimuth, elevation(ref to radius in PRM), lon, lat [ASCII default] 
        def calculate_block_extent(iy, ix):
            dask_block_azi = trans_dat.azi.data.blocks[iy, ix]
            dask_block_rng = trans_dat.rng.data.blocks[iy, ix]
            azi_allnans = dask.array.isnan(dask_block_azi).all()
            rng_allnans = dask.array.isnan(dask_block_rng).all()
            if azi_allnans or rng_allnans:
                return
            return dask.compute(iy, ix,
                                dask.array.nanmin(dask_block_azi), dask.array.nanmax(dask_block_azi),
                                dask.array.nanmin(dask_block_rng), dask.array.nanmax(dask_block_rng),
                                )

        # trans.dat - file generated by llt_grid2rat (r a topo lon lat)"
        trans_dat = self.get_trans_dat(subswath)
        # process all the chunks
        trans_blocks_ys, trans_blocks_xs = trans_dat.ll.data.numblocks
        #print ('trans_blocks_ys, trans_blocks_xs', trans_blocks_ys, trans_blocks_xs)
        with self.tqdm_joblib(tqdm(desc='Analyze Transform Blocks', total=trans_blocks_ys*trans_blocks_xs)) as progress_bar:
            extents = joblib.Parallel(n_jobs=-1)(joblib.delayed(calculate_block_extent)(iy, ix) \
                                                     for iy in range(trans_blocks_ys) for ix in range(trans_blocks_xs))
        # merge the results
        extents = np.asarray([extent for extent in extents if extent is not None])
        return extents
    
    def get_trans_dat(self, subswath=None):
        import xarray as xr

        subswath = self.get_subswath(subswath)
        filename = self.get_filenames(subswath, None, 'trans')
        trans = xr.open_dataset(filename, engine=self.engine, chunks=self.chunksize)
        return trans

    def trans_dat(self, subswath=None, interactive=False):
        import dask
        import xarray as xr
        import numpy as np
        import os
    
        # range, azimuth, elevation(ref to radius in PRM), lon, lat [ASCII default] 
        llt2rat_map = {0: 'rng', 1: 'azi', 2: 'ele', 3: 'll', 4: 'lt'}

        # build trans.dat
        def SAT_llt2rat(z, lat, lon, subswath):
            coords_ll = np.column_stack([lon.ravel(), lat.ravel(), z.ravel()])
            # for binary=True values outside of the scene missed and the array is not complete
            coords_ra = self.PRM(subswath).SAT_llt2rat(coords_ll, precise=1, binary=False)\
                .astype(np.float32).reshape(z.shape[0], z.shape[1], 5)
            return coords_ra

        dem = self.get_dem(geoloc=True)
        # prepare lazy coordinate grids
        lats = xr.DataArray(dem.lat.astype(np.float32).chunk(-1))
        lons = xr.DataArray(dem.lon.astype(np.float32).chunk(-1))
        lats, lons = xr.broadcast(lats, lons)
        _, lats, lons = xr.unify_chunks(dem, lats, lons)
        assert dem.chunks == lats.chunks and dem.chunks == lons.chunks, 'Chunks are not equal'

        # xarray wrapper
        raell = xr.apply_ufunc(
            SAT_llt2rat,
            dem,
            lats,
            lons,
            dask='parallelized',
            vectorize=False,
            output_dtypes=[np.float32],
            output_core_dims=[['raell']],
            dask_gufunc_kwargs={'output_sizes': {'raell': 5}},
            kwargs={'subswath': subswath}
        )
        # transform to separate variables
        trans = xr.Dataset({val: raell[...,key] for (key, val) in llt2rat_map.items()})

        if interactive:
            return trans

        # save to NetCDF file
        filename = self.get_filenames(subswath, None, 'trans')
        if os.path.exists(filename):
            os.remove(filename)
        encoding = {val: self.compression for (key, val) in llt2rat_map.items()}
        handler = trans.to_netcdf(filename,
                                        encoding=encoding,
                                        engine=self.engine,
                                        compute=False)
        return handler
    
    def trans_dat_parallel(self, interactive=False):
        import numpy as np
        import xarray as xr
        import dask
        from pygmtsar import tqdm_dask

        # process all the subswaths
        subswaths = self.get_subswaths()
        delayeds = []
        for subswath in subswaths:
            delayed = self.trans_dat(subswath=subswath, interactive=interactive)
            delayeds.append(delayed)

        if not interactive:
            tqdm_dask(dask.persist(delayeds), desc='Radar Transform Computing')
        else:
            return delayeds[0] if len(delayeds)==1 else delayeds

    def topo_ra(self, subswath=None, idec=2, jdec=2, n_jobs=-1, interactive=False):
        from scipy.spatial import cKDTree
        import dask
        import xarray as xr
        import numpy as np
        import os

        # trans.dat - file generated by llt_grid2rat (r a topo lon lat)"
        trans_dat = self.get_trans_dat(subswath)
    
        trans_blocks_extents = self.get_trans_dat_blocks_extents(subswath, n_jobs=n_jobs)

        # define topo_ra grid
        XMAX, yvalid, num_patch = self.PRM(subswath).get('num_rng_bins', 'num_valid_az', 'num_patches')
        YMAX = yvalid * num_patch
        #print ('DEBUG: XMAX', XMAX, 'YMAX', YMAX)
        # use center pixel GMT registration mode
        rngs = np.arange(1, XMAX+1, idec, dtype=np.int32)
        azis = np.arange(1, YMAX+1, jdec, dtype=np.int32)
        # do not use coordinate names y,x, because the output grid saved as (y,y) in this case...
        azis = xr.DataArray(azis, dims=['a'], coords={'a': azis}).chunk(self.chunksize)
        rngs = xr.DataArray(rngs, dims=['r'], coords={'r': rngs}).chunk(self.chunksize)
        azi, rng = [da.chunk(self.chunksize) for da in xr.broadcast(azis, rngs)]

        def calc_topo_ra(azi, rng):
            # check thr arguments
            assert azi.shape == rng.shape, f'ERROR: {azi.shape} != {rng.shape}'
        
            # check the selected area bounds
            ymin, ymax, xmin, xmax = azi.min(), azi.max(), rng.min(), rng.max()
            #print ('ymin, ymax', ymin, ymax, 'xmin, xmax', xmin, xmax)
            # define corresponding trans_dat blocks
            ymask = (trans_blocks_extents[:,3]>=ymin-jdec)&(trans_blocks_extents[:,2]<=ymax+jdec)
            xmask = (trans_blocks_extents[:,5]>=xmin-idec)&(trans_blocks_extents[:,4]<=xmax+idec)
            blocks = trans_blocks_extents[ymask&xmask]
            #print ('trans_dat blocks', blocks.astype(int))

            blocks_azis = []
            blocks_rngs = []
            blocks_eles = []
            for iy, ix in blocks[:,:2].astype(int):
                #print ('iy, ix', iy, ix)
                block_azi = trans_dat.azi.data.blocks[iy, ix]
                block_rng = trans_dat.rng.data.blocks[iy, ix]
                block_ele = trans_dat.ele.data.blocks[iy, ix]
                #print ('block_ele', block_ele.shape)

                blocks_azis.append(block_azi.reshape(-1))
                blocks_rngs.append(block_rng.reshape(-1))
                blocks_eles.append(block_ele.reshape(-1))
            blocks_azis = np.concatenate(blocks_azis)
            blocks_rngs = np.concatenate(blocks_rngs)
            blocks_eles = np.concatenate(blocks_eles)
        
            # build index tree - dask arrays computes automatically
            source_yxs = np.stack([blocks_azis, blocks_rngs], axis=1)
            tree = cKDTree(source_yxs, compact_nodes=False, balanced_tree=False)
        
            # query the index tree
            target_yxs = np.stack([azi.reshape(-1), rng.reshape(-1)], axis=1)
            d, inds = tree.query(target_yxs, k = 1, workers=1)
            # compute dask array to prevent ineffective index loockup on it
            matrix = blocks_eles.compute()[inds].reshape(azi.shape)
            return matrix

        # xarray wrapper
        topo = xr.apply_ufunc(
            calc_topo_ra,
            azi,
            rng,
            dask='parallelized',
            vectorize=False,
            output_dtypes=[np.float32],
        ).rename('topo_ra')

        if interactive:
            # do not flip vertically because it's returned as is without SBAS.get_topo_ra() function
            return topo

        # save to NetCDF file
        filename = self.get_filenames(subswath, None, 'topo_ra')
        if os.path.exists(filename):
            os.remove(filename)
        # flip vertically for GMTSAR compatibility reasons
        topo_ra = xr.DataArray(dask.array.flipud(topo), coords=topo.coords, name=topo.name)
        handler = topo_ra.to_netcdf(filename,
                                    encoding={'topo_ra': self.compression},
                                    engine=self.engine,
                                    compute=False)
        return handler

    def topo_ra_parallel(self, interactive=False):
        import numpy as np
        import xarray as xr
        import dask
        from pygmtsar import tqdm_dask

        # auto generate the trans.dat file
        self.trans_dat_parallel()

        # process all the subswaths
        subswaths = self.get_subswaths()
        delayeds = []
        for subswath in subswaths:
            delayed = self.topo_ra(subswath=subswath, interactive=interactive)
            delayeds.append(delayed)

        if not interactive:
            tqdm_dask(dask.persist(delayeds), desc='Radar Topography Computing')
        else:
            return delayeds[0] if len(delayeds)==1 else delayeds

    def get_topo_ra(self):
        import numpy as np
        import xarray as xr
        import dask.array

        def func(topo):
            # flip vertically for GMTSAR compatibility reasons
            return xr.DataArray(dask.array.flipud(topo), coords=topo.coords, attrs=topo.attrs, name=topo.name)\
                   .rename({'a': 'y', 'r': 'x'})

        topos = self.open_grids(None, 'topo_ra', func=func)

        return topos[0] if len(topos)==1 else topos

    def sat_look_parallel(self, n_jobs=-1, interactive=False, debug=False):
        import numpy as np
        import xarray as xr
        from tqdm.auto import tqdm
        import joblib
        import os

        def SAT_look(subswath, ilat, ilon):
            # lazy dask arrays
            lats, lons = xr.broadcast(coordlat[ilat], coordlon[ilon])
            data = dem.sel(lat=xr.DataArray(lats.values.ravel()),
                                 lon=xr.DataArray(lons.values.ravel()),
                                 method='nearest').compute()
            coords = np.column_stack([lons.values.ravel(), lats.values.ravel(), data.values.ravel()])
            # look_E look_N look_U
            look = self.PRM(subswath).SAT_look(coords, binary=True)\
                                     .reshape(-1,6)[:,3:].astype(np.float32)
            # prepare output as xarray dataset
            dims = ['lat', 'lon']
            coords = coords={'lat': coordlat[ilat], 'lon':coordlon[ilon]}
            look_E = xr.DataArray(look[:,0].reshape(lats.shape), dims=dims, coords=coords, name='look_E')
            look_N = xr.DataArray(look[:,1].reshape(lats.shape), dims=dims, coords=coords, name='look_N')
            look_U = xr.DataArray(look[:,2].reshape(lats.shape), dims=dims, coords=coords, name='look_U')
            return xr.merge([look_E, look_N, look_U])

        # process all the subswaths
        subswaths = self.get_subswaths()
        dss = []
        for subswath in subswaths:
            intf_ra2ll_file = os.path.join(self.basedir, f'F{subswath}_intf_ra2ll.grd')
            sat_look_file   = os.path.join(self.basedir, f'F{subswath}_sat_look.grd')

            # cleanup before creating the new file
            if not interactive and os.path.exists(sat_look_file):
                if debug:
                    print ('DEBUG: remove', sat_look_file)
                os.remove(sat_look_file)

            dem = self.get_dem(subswath, geoloc=True)
            grid_ll = xr.open_dataarray(intf_ra2ll_file, engine=self.engine, chunks=self.chunksize)
            lats, lons = grid_ll.data.numblocks
            latchunks, lonchunks = grid_ll.chunks
            coordlat = np.array_split(grid_ll.lat, np.cumsum(latchunks))
            coordlon = np.array_split(grid_ll.lon, np.cumsum(lonchunks))
            with self.tqdm_joblib(tqdm(desc=f'SAT_look Computing', total=lats*lons)) as progress_bar:
                ds = joblib.Parallel(n_jobs=n_jobs)(joblib.delayed(SAT_look)(subswath, ilat, ilon) \
                                               for ilat in range(lats) for ilon in range(lons))
            # concatenate the chunks
            if xr.__version__ == '0.19.0':
                # for Google Colab
                ds = xr.merge(ds)
            else:
                # for modern xarray versions
                ds = xr.combine_by_coords(ds)

            if interactive:
                dss.append(ds)
            else:
                if debug:
                    print ('DEBUG: write NetCDF', sat_look_file)
                # magic: add GMT attribute to prevent coordinates shift for 1/2 pixel
                ds.attrs['node_offset'] = 1
                # save to NetCDF file
                ds.to_netcdf(sat_look_file,
                             encoding={var:self.compression for var in ds.data_vars},
                             engine=self.engine)
        if not interactive:
            return
        return dss[0] if len(dss)==1 else dss

    def get_sat_look(self, subswath=None):
        import xarray as xr
        import os

        if subswath is None:
            # process all the subswaths
            subswaths = self.get_subswaths()
        else:
            # process only one subswath
            subswaths = [subswath]

        sat_looks = []
        for subswath in subswaths:
            sat_look_file = os.path.join(self.basedir, f'F{subswath}_sat_look.grd')
            assert os.path.exists(sat_look_file), 'ERROR: satellite looks grid missed. Build it first using SBAS.sat_look_parallel()'
            sat_look = xr.open_dataset(sat_look_file, engine=self.engine, chunks=self.chunksize)
            sat_looks.append(sat_look)

        return sat_looks[0] if len(sat_looks)==1 else sat_looks

    def geocode_parallel(self, subswath=None, pairs=None, debug=False):
    
        assert pairs is not None or subswath is not None, 'ERROR: define pairs argument'
        if pairs is None and subswath is not None:
            pairs = subswath
            subswath = None
        
        subswath = self.get_subswath(subswath)
        if debug:
            print (f'DEBUG: build translation matrices for direct and inverse geocoding for subswath {subswath}')

        # build a new trans_dat for merged subswaths only
        if len(str(subswath)) > 1:
            self.topo_ra_parallel()

        # build DEM grid coordinates transform matrix
        self.ra2ll(subswath, debug=debug)
    
        # transforms for interferogram grid
        grids = self.open_grids(pairs[:1], 'phasefilt')
        # build radar coordinates transformation matrix for the interferograms grid stack
        self.intf_ra2ll_matrix(subswath, grids, debug=debug)
        # build geographic coordinates transformation matrix for landmask and other grids
        self.intf_ll2ra_matrix(subswath, grids, debug=debug)
